---
title: "Stat435_HW1"
author: "Su Huang"
date: "4/4/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
```

### problem 1

1. Suppose that you are interested in performing regression on a particular dataset,
in order to answer a particular scientific question. You need to decide whether
to take a parametric or a non-parametric approach.
(a) In general, what are the pros and cons of taking a parametric versus a
non-parametric approach? 

    The pros of parametric approach: it reduces the problem of estimating f down to one of estimating a set of parameters; Less data needed; easy and faster to interpret.

    The cons of parametric approach: the model we choose may not fit into the true known form of f which will lead to large error and poor estimate. And fitting a more flexible model requires more number of parameters and generate overfitting.

    The pros of non-parametric approach: avoid the assumption of a functional form for f, they have the potential to accurately fit a wider range of possible shapes for f.

    The cons of non-parametric approach: since they do not reduce the problem of estimating f to a small number of parameters, more data and longer time is required for estimate; 

(b) What properties of the data or scientific question would lead you to take
a parametric approach?

    The question has small size of data and look for a understandable and interpretable prediction.

(c) What properties of the data or scientific question would lead you to take
a non-parametric approach?

    The questions we don't want to make improper assumptions, no requirement on efficiency and have large size of data.

### problem 2
2. In each setting, would you generally expect a flexible or an inflexible statistical
machine learning method to perform better? Justify your answer.
(a) Sample size n is very small, and number of predictors p is very large.

    Inflexible method is better because the probability of overfitting will be high with a small sample size and large p. Small size n is not enough for flexible method.

(b) Sample size n is very large, and number of predictors p is very small.

    Flexible method is better because it could fit data better and large sample size promotes the better prediction.

(c) Relationship between predictors and response is highly non-linear.

    Flexible method is better because it's necessary to use a flexible method to find non-linear effect.

(d) The variance of the error terms, i.e. œÉ2 = Var(), is extremely high.

    Inflexible method is better because when we have high variance, the flexible method will try to learn and apply the noise term which may bring more test error.

## problem 3
3. For each scenario, determine whether it is a regression or a classification prob-
lem, determine whether the goal is inference or prediction, and state the values
of n (sample size) and p (number of predictors).
(a) I want to predict each student‚Äôs final exam score based on his or her
homework scores. There are 50 students enrolled in the course, and each
student has completed 8 homeworks.

    Regression and prediction; n=50, p=8. The score is quantitative and we wants to predict score.

(b) I want to understand the factors that contribute to whether or not a
student passes this course. The factors that I consider are (i) whether or
not the student has previous programming experience; (ii) whether or not
the student has previously studied linear algebra; (iii) whether or not the
student has taken a previous stats/probability course; (iv) whether or not
the student attends office hours; (v) the student‚Äôs overall GPA; (vi) the
student‚Äôs year (e.g. freshman, sophomore, junior, senior, or grad student).
I have data for all 50 students enrolled in the course.

    Classification and inference; n=50, p=6. These factors are not quantitative.
    
## problem 4
4. This problem has to do with the bias-variance trade-off and related ideas, in the
context of regression. For (a) and (b), it‚Äôs okay to submit hand-sketched plots:
you are not supposed to actually compute the quantities referred to below on
data; instead, this is a thought exercise.
(a) Make a plot, like the one we saw in class, with ‚Äúflexibility‚Äù on the x-
axis. Sketch the following curves: squared bias, variance, irreducible error,
reducible error, expected prediction error. Be sure to label each curve.
Indicate which level of flexibility is ‚Äúbest‚Äù.
![](4a.jpeg)
The best flexibility level the lowest expected prediction error.

(b) Make a plot with ‚Äúflexibility‚Äù on the x-axis. Sketch curves corresponding
to the training error and the test error. Be sure to label each curve.
Indicate which level of flexibility is ‚Äúbest‚Äù.
![](4b.jpeg)
The best flexibility level is when we have the smallest test error.

(c) Describe an ÀÜf that has extremely low bias, and extremely high variance.
Explain your answer.

    more flexible methods have low bias because it can fit into the data easily and higher variance because changing any data will cause f change considerably

(d) Describe an ÀÜf that has extremely high bias, and zero variance. Explain
your answer.

    inflexible methods has high bias because inflexible method might not be able to fit the data properly in many cases and low variance because moving any single data will likely cause only a small change of the whole prediction.


## problem 5
We now consider a classification problem. Suppose we have 2 classes (labels),
25 observations per class, and p = 2 features. We will call one class the ‚Äúred‚Äù
class and the other class the ‚Äúblue‚Äù class. The observations in the red class
are drawn i.i.d. from a Np(Œºr,I) distribution, and the observations in the blue
class are drawn i.i.d. from a Np(Œºb,I) distribution, where Œºr =(0 0)
is the mean in the red class, and where Œºb = (1.5 1.5) is the mean in the blue class.
(a) Generate a training set, consisting of 25 observations from the red class
and 25 observations from the blue class. (You will want to use the R
function rnorm.) Plot the training set. Make sure that the axes are
properly labeled, and that the observations are colored according to their
class label.

```{r}
library(ggplot2)
set.seed(435)

red <- data.frame(X1 = rnorm(25, 0, 1), X2 = rnorm(25, 0, 1)) %>%
  mutate(color = "red")

blue <- data.frame(X1 = rnorm(25, 1.5, 1), X2 = rnorm(25, 1.5, 1)) %>%
  mutate(color = "blue")

train <- rbind(red, blue)

plot(train$X1, train$X2, xlab="x1",ylab="x2",col=train$color,xlim=c(-1.9,3), ylim=c(-2,3.7))
legend(x="topleft",  legend=c("red class", "blue class"),
       col=c("red", "blue"),
       pch = c(1,1)
       )
```


(b) Now generate a test set consisting of 25 observations from the red class
and 25 observations from the blue class. On a single plot, display both the training and test set, using one symbol to indicate training observations
(e.g. circles) and another symbol to indicate the test observations (e.g.
squares). Make sure that the axes are properly labeled, that the symbols
for training and test observations are explained in a legend, and that the
observations are colored according to their class label.

```{r}
red_test <- data.frame(X1 = rnorm(25, 0, 1), X2 = rnorm(25, 0, 1)) %>%
  mutate(color = "red")

blue_test <- data.frame(X1 = rnorm(25, 1.5, 1), X2 = rnorm(25, 1.5, 1)) %>%
  mutate(color = "blue")

test <- rbind(red_test, blue_test)


plot(train$X1, train$X2, xlab="x1",ylab="x2",col=train$color,xlim=c(-1.9,3), ylim=c(-2,3.6))
points(test$X1, test$X2, col = test$color, pch=5)
legend(x="topleft",  legend=c("red train", "blue train"),
       col=c("red", "blue"),
       pch = c(1,1),cex=1
       )
legend(x=-2.1, y= 2.5,  legend=c("red test", "blue test "),
       col=c("red", "blue"),
       pch = 5, cex=1
       )

```



(c) Using the knn function in the library class, fit a k-nearest neighbors
model on the training set, for a range of values of k from 1 to 20. Make a
plot that displays the value of 1/k on the x-axis, and classification error
(both training error and test error) on the y-axis. Make sure all axes and
curves are properly labeled. Explain your results.


```{r} 
library(class)
result <- data.frame(k = rep(0,20), train_error = rep(0,20), test_error = rep(0,20))
trai <- train[,1:2]
tes <- test[,1:2]

for (i in 1:20){
  
  train_k <- class::knn(trai, trai, train$color, k=i)
  train_error <- mean(train_k != train$color)
  
  test_k <- class::knn(trai, tes, train$color, k=i)
  test_error <- mean(test_k != test$color)
  
  result[i,"k"] <- i
  result[i,"train_error"] <- train_error
  result[i, "test_error"] <- test_error
}

plot(1/result$k, result$train_error, type="b", col = "red", xlim = c(0, 1.1), ylim = c(0, 0.3), xlab ="1/k", ylab="error" )
points(1/result$k, result$test_error, type="b", col = "blue")
legend(x="right",  legend=c("train error", "test error"),
       col=c("red", "blue"),
       pch = c(1,1),cex=1)
```

    We have the smallest error when 1/k is less than 0.2, which means quite large k and small k are both not appropriate for knn. We have a train error equals 0 because when k = 1, it compares to itself. 


(d) For the value of k that resulted in the smallest test error in part (c)
above, make a plot displaying the test observations as well as their true
and predicted class labels. Make sure that all axes and points are clearly
labeled.

```{r}
min_k<-result$k[result$test_error==min(result$test_error)]

plot(test$X1, test$X2, col = test$color, pch=5, xlab = "X1", ylab = "X2")
points(test$X1, test$X2, col = class::knn(trai, tes, train$color, k=min_k), pch=3)
legend(x="topleft",  legend=c("red test", "blue test", "red pred", "blue pred"),
       col=c("red", "blue", "red", "blue"),
       pch = c(5,5,3,3),cex=1)
```


(e) Recall that the Bayes classifier assigns an observation to the red class if
Pr(Y = red|X = x) > 0.5, and to the blue class otherwise. The Bayes
error rate is the error rate associated with the Bayes classifier. What is
the value of the Bayes error rate in this problem? Explain your answer.

```{r}
#train$redp <- dnorm(train$X1) * dnorm(train$X2)
#train$bluep <- dnorm(train$X1, 1.5) * dnorm(train$X2, 1.5)
B <- table(class::knn(trai, tes, train$color, k=min_k), test$color)
1-sum(diag(B))/sum(B)
```
The bayes error rate is 1-E(maxP(Y=j|X)). The bayes error rate is 0.18, which match what we get on c and d since most prediction are correct.

6. We will once again perform k-nearest-neighbors in a setting with p = 2 features.
But this time, we‚Äôll generate the data differently: let X1 ‚àº Unif[0,1] and
X2 ‚àºUnif[0,1], i.e. the observations for each feature are i.i.d. from a uniform
distribution. An observation belongs to class ‚Äúred‚Äù if (X1‚àí0.5)2+(X2‚àí0.5)2 >
0.15 and X1 > 0.5; to class ‚Äúgreen‚Äù if (X1 ‚àí0.5)2 + (X2 ‚àí0.5)2 > 0.15 and
X1 ‚â§0.5; and to class ‚Äúblue‚Äù otherwise.
(a) Generate a training set of n = 200 observations. (You will want to use
the R function runif.) Plot the training set. Make sure that the axes are
properly labeled, and that the observations are colored according to their
class label.

```{r}
data <- data.frame(X1 = runif(200, 0, 1), X2 = runif(200, 0, 1)) %>%
  mutate(Color = ifelse((X1-0.5)^2+(X2-0.5)^2 > 0.15,
                    ifelse(X1 > 0.5, "red", "green"), "blue"))

plot(data$X1, data$X2, xlab="x1", ylab="x2", col=data$Color, xlim = c(-0.25,1))
legend(x="topleft",  legend=c("red class", "blue class", "green class"),
       col=c("red", "blue", "green"),
       pch = 1
       )
```


(b) Now generate a test set consisting of another 200 observations. On a single
plot, display both the training and test set, using one symbol to indicate
training observations (e.g. circles) and another symbol to indicate the
test observations (e.g. squares). Make sure that the axes are properly
labeled, that the symbols for training and test observations are explained
in a legend, and that the observations are colored according to their class
label.

```{r}
data_test <- data.frame(X1 = runif(200, 0, 1), X2 = runif(200, 0, 1)) %>%
  mutate(Color = ifelse((X1-0.5)^2+(X2-0.5)^2 > 0.15,
                    ifelse(X1 > 0.5, "red", "green"), "blue"))

plot(data$X1, data$X2, xlab="x1", ylab="x2", col=data$Color, pch = 1, xlim = c(-0.25,1))
points(data_test$X1, data_test$X2, col=data_test$Color, pch = 2)
legend(x="topleft",  legend=c("red class", "blue class", "green class", "red test", "blue test", "green test"),
       col=c("red", "blue", "green", "red", "blue", "green"),
       pch = c(1,1,1,2,2,2)
       )
```


(c) Using the knn function in the library class, fit a k-nearest neighbors
model on the training set, for a range of values of k from 1 to 50. Make a
plot that displays the value of 1/k on the x-axis, and classification error
(both training error and test error) on the y-axis. Make sure all axes and
curves are properly labeled. Explain your results.

```{r}
result2 <- data.frame(k = rep(0,50), train_error=rep(0,50), test_error=rep(0,50))
data3 <- data[,1:2]
data_test3 <- data_test[,1:2]

for (i in 1:50){
  
  train_k <- class::knn(data3, data3, data$Color, k=i)
  train_error <- mean(train_k != data$Color)
  
  test_k <- class::knn(data3, data_test3, data$Color, k=i)
  test_error <- mean(test_k != data_test$Color)
  
  result2[i,"k"] <- i
  result2[i,"train_error"] <- train_error
  result2[i, "test_error"] <- test_error
}

plot(1/result2$k, result2$train_error, type="b", col = "red", ylim = c(0, 0.15), xlab ="1/k", ylab="error" )
points(1/result2$k, result2$test_error, type="b", col = "blue")
legend(x="topright",  legend=c("train error", "test error"),
       col=c("red", "blue"),
       pch = c(1,1),cex=1)
```
  
    I got a bit lower error than the question 5. The min test error data is around 0.3. The error decreases sharply when 1/k increases from 0 to 0.2. 

(d) For the value of k that resulted in the smallest test error in part (c)
above, make a plot displaying the test observations as well as their true
and predicted class labels. Make sure that all axes and points are clearly
labeled.
```{r}
min_k2<-result2$k[result2$test_error == min(result2$test_error)]

plot(data_test$X1, data_test$X2, col = data_test$Color, pch = 5, xlab = "X1", ylab = "X2", xlim = c(-0.25,1))
points(data_test$X1, data_test$X2, col = class::knn(data3, data_test3, data$Color, k=min_k2), pch = 3)
legend(x ="topleft", legend = c("red test", "blue test", "green test", "red pred", "blue pred", "green pred"),
       col = c("red", "blue","green", "red", "blue","green"),
       pch = c(5,5,5,3,3,3), cex = 1)
```



(e) In this example, what is the Bayes error rate? Justify your answer, and
explain how it relates to your findings in (c) and (d).

```{r}
B <- table(class::knn(data3, data_test3, data$Color, k=min_k2), data_test$Color)
1-sum(diag(B))/sum(B)
```
The error rate is 0.61 since we have made many wrong predictions in c and d. When we have lower accuracy for prediction, we will have larger bayes error rate. 

7. This exercise involves the Boston housing data set, which is part of the ISLR2
library.

(a) How many rows are in this data set? How many columns? What do the
rows and columns represent?
```{r}
library(ISLR2)
ncol(Boston)
nrow(Boston)
```

    There are 13 columns, and 506 rows.
    The row represents the number of house we have. 
    The column represents the factors or the parameters that are contained for each house.

(b) Make some pairwise scatterplots of the predictors (columns) in this data
set. Describe your findings.
```{r}
plot(Boston$lstat, Boston$medv,
     main="Relationship between lstat and medv",
     xlab="lstat", ylab="medv")
```

    Istat and medv has negative relationship. Medc decreases when the lstat increases.

```{r}
plot(Boston$age, Boston$rm,
     main="Relationship between rm and age",
     xlab="age", ylab="rm")
```

    The room number is pretty consistent around 6 or 7 regardless of the house age.

```{r}
plot(Boston$crim, Boston$dis,
     main="Relationship between crim and medv",
     xlab="crim", ylab="medv", xlim = c(0,1))
```

    Lower crim intend to have a bit higher medv

(c) Are any of the predictors associated with per capita crime rate? If so,
explain the relationship.
```{r}
plot(Boston$age, Boston$crim,
     main="Relationship between crim and age",
     xlab="age", ylab="crim")
```

    Yes, the crim predictors is associated with per capita crime rate by age of the house.The crim is larger when the house is old. The relationship between crim and medv is the town with high crim rate intend to have lower medv. But the low crim rate doesn't guarantee the high medv.

(d) Do any of the suburbs of Boston appear to have particularly high crime
rates? Tax rates? Pupil-teacher ratios? Comment on the range of each
predictor.

```{r}
range(Boston$crim)
range(Boston$tax)
range(Boston$ptratio)
```

    The range of crime rates is between 0.00632 and 88.9762
    The range of tax rates is between 187 and 711
    The range of pupil teacher ratios is between 12.6 and 22

(e) How many of the suburbs in this data set bound the Charles river?

```{r}
nrow(Boston[Boston$chas == 1,])
```

    There are 35 suburbs bounding the Charles river.

(f) What are the mean and standard deviation of the pupil-teacher ratio
among the towns in this data set?
```{r}
mean(Boston$ptratio)
sd(Boston$ptratio)
```

    The mean of the pupil-teacher ratio is 18.45553
    The sd is 2.164946

(g) Which suburb of Boston has highest median value of owner-occupied
homes? What are the values of the other predictors for that suburb, and
how do those values compare to the overall ranges for those predictors?
Comment on your findings.

```{r}
Boston[Boston$medv == max(Boston$medv),]
```
   
    There are 16 of them having highest medv. Other predictors variate depending on the specific suburbs. I didnt see any predictors have an consistently extreme values among these suburbs.

(h) In this data set, how many of the suburbs average more than six rooms per
dwelling? More than eight rooms per dwelling? Comment on the suburbs
that average more than eight rooms per dwelling.

```{r}
library(dplyr)
nrow(Boston[Boston$rm > 6,]) 
nrow(Boston[Boston$rm > 8,])
```

There are 333 suburbs that have more than 6 rooms per dwelling and 13 suburbs that have more than 8. 